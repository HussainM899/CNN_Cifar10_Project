{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIWoshJ2zM5c"
      },
      "source": [
        "# **Convolutional Neural Network CNN - CIFAR10 Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z72QA8sXzM5j"
      },
      "source": [
        "- ### **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bwmZNRVBC_VI"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5KHHKYPJDHfX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29Wdpek4zM5n"
      },
      "source": [
        "- ### **Data Preparation**\n",
        "This section covers loading the CIFAR-10 dataset, scaling the images for our model, and preparing our labels for training. We aim for our model to understand and predict these labels accurately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qf5nNBjDgHV",
        "outputId": "32016d2b-3579-4ab4-ca3d-5d7c14dd4b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Tb-N89giDhxQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbtZ1a2ZD5Pz",
        "outputId": "ad5a34fb-0fac-4271-9dcc-41b0c72a56fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the shape of input features\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "i8C2x5waD7bS",
        "outputId": "1cc16907-2517-4fa3-8bc4-df473bf04c34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ae20af3d7b0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display Image\n",
        "\n",
        "single_image  = x_train[30]\n",
        "\n",
        "plt.imshow(single_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXSoI65dEGew",
        "outputId": "666f1838-ca50-43c1-d8b5-8e64d36d6f6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[6],\n",
              "       [9],\n",
              "       [9],\n",
              "       ...,\n",
              "       [9],\n",
              "       [1],\n",
              "       [1]], dtype=uint8)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48qdYMylEoL1",
        "outputId": "484e43e4-bd23-4dbf-ba1f-dda68a9a7622"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[3],\n",
              "       [8],\n",
              "       [8],\n",
              "       ...,\n",
              "       [5],\n",
              "       [1],\n",
              "       [7]], dtype=uint8)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jAEcbYxwEpzW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwJ0XKb5FBQM",
        "outputId": "e7f16042-63d5-4d26-f7a5-b099bffa5498"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk9PTtzwzM5w"
      },
      "source": [
        "- ### **Why Use `to_categorical`?**\n",
        "The `to_categorical` method transforms integer labels into a one-hot encoded format, crucial for multi-class classification in neural networks. This encoding matches the softmax output of the network's final layer, simplifying loss calculations and optimizing model training. It ensures each label is represented as a vector, indicating the class presence with a 1 in the corresponding position, making it compatible with the expected format for neural network outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-N12pdGFGwt",
        "outputId": "abc03044-6fc0-40dd-9959-d1dd82d007db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_example = to_categorical(y_train)\n",
        "\n",
        "y_example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Ha_afJGIJ3",
        "outputId": "88cca9ec-87fc-4501-a8e9-4b14ce8d5c5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_example.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYBmqGCVGPEJ",
        "outputId": "79aa5139-4422-4661-c049-a97235ba2478"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_cat_train = to_categorical(y_train, 10)\n",
        "y_cat_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIAaQM5nH3u2",
        "outputId": "4f574f83-2dd8-4fbb-ba94-63e616424833"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_cat_test = to_categorical(y_test, 10)\n",
        "y_cat_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-XirM-LH_AG",
        "outputId": "20ee9170-22a8-4c53-83e7-30ba08b9a7b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "241"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This gives the max pixel value present in this picture\n",
        "\n",
        "single_image.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB6FmABHIC5H",
        "outputId": "8392f523-3f93-445b-a240-a6130a4489ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This gives the min pixel value present in this picture\n",
        "\n",
        "single_image.min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhwbuyxfzM52"
      },
      "source": [
        "- ### **Scaling the input features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LmM0aJ-3IMOb"
      },
      "outputs": [],
      "source": [
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fZjtmhKXKT1i"
      },
      "outputs": [],
      "source": [
        "scaled_image = x_train[30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TMBrLv_Kxl9",
        "outputId": "28192794-e9ce-4eff-b0e0-563918b4c840"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9450980392156862"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled_image.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrWFO3WRLYdM",
        "outputId": "b181baf6-12f1-4844-e8d6-4e23dab19fab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0196078431372549"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled_image.min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc1jTTy0zM54"
      },
      "source": [
        "- ### **CNN Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "B4EYb3lkLaNh"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "18KlGlwnLu9X"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layers with ReLU activation\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "\n",
        "# Flattening the output for the dense layers\n",
        "model.add(Flatten())\n",
        "# Dense layers with ReLU activation\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Output layer with Softmax activation\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HaJDFHshL7dc"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Seyo076XNJN2",
        "outputId": "76118446-5744-464a-88d8-e05ca2959157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 32)          18464     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                32832     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 73098 (285.54 KB)\n",
            "Trainable params: 73098 (285.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a9C3rEE7PWF"
      },
      "source": [
        "- ### **Model Architecture Summary**\n",
        "\n",
        "Our CNN model is designed to classify 32x32 pixel images from the CIFAR-10 dataset into one of 10 categories (like cars, birds, etc.). Here's the structure:\n",
        "\n",
        "- **Convolutional Layers**: Extract patterns from images using filters. We have layers with 32, 64, and again 32 filters, each 3x3 in size. More filters help in detecting complex features.\n",
        "\n",
        "- **MaxPooling Layers**: Reduce the image dimensionality to make the processing faster and more efficient without losing key features.\n",
        "\n",
        "- **Flatten Layer**: Transforms the 2D feature maps into a 1D feature vector, preparing the data for the fully connected layers that follow.\n",
        "\n",
        "- **Dense Layers**: These are the neural network's decision-makers. We start broad with 64 neurons, narrow down to 32, and finally to 10 neurons, each representing a class of the CIFAR-10 dataset.\n",
        "\n",
        "- **Output Layer with Softmax**: Gives us a probability for each of the 10 classes, indicating how likely the image belongs to each class.\n",
        "\n",
        "The model has a total of 73,098 parameters, all of which are trainable, meaning they're updated during the learning process to improve classification accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XuVVscCNKpl",
        "outputId": "c8b59903-cf20-431f-b371-37fe3a85ed1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 1.6366 - accuracy: 0.4027\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 1.2435 - accuracy: 0.5577\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 1.0777 - accuracy: 0.6201\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.9685 - accuracy: 0.6619\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.8926 - accuracy: 0.6882\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.8340 - accuracy: 0.7088\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.7827 - accuracy: 0.7280\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 41s 26ms/step - loss: 0.7434 - accuracy: 0.7399\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.7111 - accuracy: 0.7556\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.6809 - accuracy: 0.7652\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ae1a4b7aa70>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_cat_train, epochs = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOtZVFyo7vW2"
      },
      "source": [
        "- ### **Training Performance**\n",
        "\n",
        "The model was trained for 10 epochs, showing gradual improvement in accuracy with each epoch:\n",
        "\n",
        "- **First Epoch**: The model had a low accuracy of about 40%, indicating the starting point of learning.\n",
        "- **Progress Over Epochs**: With each epoch, the model's accuracy increased as it learned from the training data, showing that it was becoming better at classifying the images.\n",
        "- **Tenth Epoch**: By the last epoch, the model achieved an accuracy of around 76%, which means it correctly classified 76% of the training images.\n",
        "\n",
        "Loss, a measure of error, decreased from approximately 1.63 to 0.68, indicating the model's predictions were getting closer to the actual labels.\n",
        "\n",
        "While accuracy improved over time, further optimization may be needed to increase accuracy and reduce loss further, suggesting the model hasn't yet reached its peak performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6lGnVdyNbAR",
        "outputId": "dc67338c-2ac7-41e9-8b6c-b16fa996aefc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 5s 16ms/step - loss: 0.9603 - accuracy: 0.6932\n",
            "Test accuracy:  0.6931999921798706\n",
            "313/313 [==============================] - 5s 17ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.70      0.71      1000\n",
            "           1       0.86      0.81      0.84      1000\n",
            "           2       0.47      0.71      0.56      1000\n",
            "           3       0.54      0.54      0.54      1000\n",
            "           4       0.69      0.64      0.66      1000\n",
            "           5       0.69      0.49      0.57      1000\n",
            "           6       0.77      0.75      0.76      1000\n",
            "           7       0.81      0.71      0.75      1000\n",
            "           8       0.69      0.89      0.78      1000\n",
            "           9       0.86      0.71      0.78      1000\n",
            "\n",
            "    accuracy                           0.69     10000\n",
            "   macro avg       0.71      0.69      0.70     10000\n",
            "weighted avg       0.71      0.69      0.70     10000\n",
            "\n",
            "[[698  15  86  12  11   4   7   2 133  32]\n",
            " [ 21 810  18   8  11   3  11   3  69  46]\n",
            " [ 62   4 711  41  63  25  43  26  21   4]\n",
            " [ 22   7 147 537  45 102  76  31  23  10]\n",
            " [ 17   2 150  61 635  27  35  48  24   1]\n",
            " [ 20   3 165 201  42 486  27  37  16   3]\n",
            " [  7   3 114  60  37   5 750   4  14   6]\n",
            " [ 23   1  91  42  69  42   9 706   8   9]\n",
            " [ 46  19  16  12   2   2   5   3 885  10]\n",
            " [ 40  74  24  21  10   4  15  17  81 714]]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_cat_test)\n",
        "print(\"Test accuracy: \", test_accuracy)\n",
        "\n",
        "# For more detailed metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Predict classes with model\n",
        "import numpy as np\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Actual classes\n",
        "true_classes = np.argmax(y_cat_test, axis=1)\n",
        "\n",
        "# Generate and print classification report and confusion matrix\n",
        "print(classification_report(true_classes, predicted_classes))\n",
        "print(confusion_matrix(true_classes, predicted_classes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCx1zr758aHM"
      },
      "source": [
        "- ### **Model Evaluation on Test Data**\n",
        "\n",
        "The model's performance was evaluated on the test set, which is a separate dataset not seen by the model during training. The purpose is to assess how well the model generalizes to new data.\n",
        "\n",
        "- **Test Accuracy**: The model achieved a test accuracy of approximately 69.32%. This means that it correctly predicted the class of an image about 69% of the time on the test dataset.\n",
        "\n",
        "- **Precision and Recall**: The classification report provides precision (the ratio of correctly predicted positive observations to the total predicted positives) and recall (the ratio of correctly predicted positive observations to all actual positives) for each class:\n",
        "  - Classes with higher precision and recall are better predicted by the model.\n",
        "  - Classes with lower scores may need more data or model adjustments to improve predictions.\n",
        "\n",
        "- **F1-Score**: This is the harmonic mean of precision and recall and gives a combined idea about the false positives and false negatives. An F1-score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
        "\n",
        "- **Confusion Matrix**: It shows the actual versus predicted classifications. For instance, the number of times the model predicted each class compared to the actual class labels. This can help identify which classes are being confused with others.\n",
        "\n",
        "The provided metrics are key to understanding the model's strengths and weaknesses in classifying the CIFAR-10 dataset. To further improve the model, you may consider refining the model architecture, training for more epochs, or using techniques like data augmentation and hyperparameter tuning.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
